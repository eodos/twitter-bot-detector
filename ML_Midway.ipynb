{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Make the data set and labels\n",
    "# Create feature vector num. of samples x num. of features\n",
    "#X= np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n",
    "# Create feature vector num. of samples x 1 (bot or not)\n",
    "#y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the Data is Processed the data is 75/25 split for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data set for all classifiers first\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "y_b = np.ones(len(X_b[1]))\n",
    "y_n = np.zeros(len(X_n[1]))\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.25, random_state=42)\n",
    "X_train.extend(X_train_b)\n",
    "X_test.extend(X_test_b)\n",
    "y_train.extend(y_train_b)\n",
    "y_test.extend(y_test_b)\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_n, y_n, test_size=0.25, random_state=24)\n",
    "X_train.extend(X_train_n)\n",
    "X_test.extend(X_test_n)\n",
    "y_train.extend(y_train_n)\n",
    "y_test.extend(y_test_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Naive Bayes for tweets\n",
    "# Create a Gaussian Classifier\n",
    "clf_NB = GaussianNB()\n",
    "# Train the model using the training sets \n",
    "clf_NB.fit(X_train, y_train)\n",
    "#Predict Output \n",
    "predicted_NB = clf_NB.predict(X_test)\n",
    "y_true = y_test\n",
    "y_pred = predicted_NB\n",
    "print('For Gaussian Naive Bayes:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decison tree for tweets\n",
    "# Think about using PCA to simplfy data and have better DT\n",
    "# To find best Tree depth\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "# Should test depth from 3 to num. of features\n",
    "param_grid = {'max_depth': np.arange(3, len(X_train[1])), 'min_samples_split':(len(y_train)(np.arange(0,10,.5)/100))}\n",
    "clf_DT = GridSearchCV(tree.DecisionTreeClassifier(), param_grid)\n",
    "clf_DT = clf_DT.fit(X_train, y_train)\n",
    "#predicted_DT = clf_DT.predict_proba(xtest)[:, 1]\n",
    "predicted_DT = clf_DT.predict(X_test)\n",
    "best_set = clf_DT.best_params_\n",
    "y_true = y_test\n",
    "y_pred = predicted_DT\n",
    "print('For Decision Tree:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))\n",
    "# To visualize the tree\n",
    "from IPython.display import Image  \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                     feature_names=iris.feature_names,  \n",
    "                     class_names=iris.target_names,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_RF = RandomForestClassifier()\n",
    "param_grid = {\"max_depth\": [np.arange(3, len(X_train[1])), None],\n",
    "              \"min_samples_split\": (len(y_train)(np.arange(0,11,.5)/100))\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "grid_search_RF = GridSearchCV(clf_RF, param_grid=param_grid)\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "predicted_RF = grid_search_RF.predict(X_test)\n",
    "y_pred = predicted_RF\n",
    "print('For Random Forest:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))\n",
    "best_set_RF = grid_search_RF.best_params_\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "report(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "# recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], \n",
    "# or standardize it to have mean 0 and variance 1\n",
    "from sklearn.svm import SVC\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['poly'], 'degree' : np.arange(1,6), 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}\n",
    "                    {'kernel': ['sigmoid'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}]\n",
    "clf_SVM = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "clf_SVM.fit(X_train, y_train)\n",
    "best_set_SVM = clf_SVM.best_params_\n",
    "predicted_SVM = clf_SVM.predict(X_test)\n",
    "y_pred = predicted_SVM\n",
    "print('For SVM:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EnsembleSVM\n",
    "# Want to randomly sample data np.random.shuffle(x)\n",
    "# And then trian multiple SVM's \n",
    "# Hard vote on a label\n",
    "from math import floor\n",
    "n_sets = 10\n",
    "n_size = floor(len(X_train)/n_sets)\n",
    "svm1 = SVC(best_set_SVM, random_state=1)\n",
    "svm2 = SVC(best_set_SVM, random_state=2)\n",
    "svm3 = SVC(best_set_SVM, random_state=3)\n",
    "svm4 = SVC(best_set_SVM, random_state=4)\n",
    "svm5 = SVC(best_set_SVM, random_state=5)\n",
    "svm6 = SVC(best_set_SVM, random_state=6)\n",
    "svm7 = SVC(best_set_SVM, random_state=7)\n",
    "svm8 = SVC(best_set_SVM, random_state=8)\n",
    "svm9 = SVC(best_set_SVM, random_state=9)\n",
    "svm10 = SVC(best_set_SVM, random_state=10)\n",
    "svm1.fit(X_train[0:n_size],y_train[0:n_size])\n",
    "svm2.fit(X_train[n_size:2*nsize],y_train[n_size:2*nsize])\n",
    "svm3.fit(X_train[2*n_size:3*nsize],y_train[2*n_size:3*nsize])\n",
    "svm4.fit(X_train[3*n_size:4*nsize],y_train[3*n_size:4*nsize])\n",
    "svm5.fit(X_train[4*n_size:5*nsize],y_train[4*n_size:5*nsize])\n",
    "svm6.fit(X_train[5*n_size:6*nsize],y_train[5*n_size:6*nsize])\n",
    "svm7.fit(X_train[6*n_size:7*nsize],y_train[6*n_size:7*nsize])\n",
    "svm8.fit(X_train[7*n_size:7*nsize],y_train[7*n_size:8*nsize])\n",
    "svm9.fit(X_train[8*n_size:9*nsize],y_train[8*n_size:9*nsize])\n",
    "svm10.fit(X_train[9*n_size:10*nsize],y_train[9*n_size:10*nsize])\n",
    "vt1 = svm1.predict(X_test)\n",
    "vt2 = svm2.predict(X_test)\n",
    "vt3 = svm3.predict(X_test)\n",
    "vt4 = svm4.predict(X_test)\n",
    "vt5 = svm5.predict(X_test)\n",
    "vt6 = svm6.predict(X_test)\n",
    "vt7 = svm7.predict(X_test)\n",
    "vt8 = svm8.predict(X_test)\n",
    "vt9 = svm9.predict(X_test)\n",
    "vt10 = svm10.predict(X_test)\n",
    "vt_count = np.array(vt1)+np.array(vt2)+np.array(vt3)+np.array(vt4)+np.array(vt5)+np.array(vt6)+np.array(vt7)+np.array(vt8)+np.array(vt9)+np.array(vt10)\n",
    "predicted_ES = [for i in range(0,len(vt_count)) vt_count[i]>=5]\n",
    "y_pred = predicted_ES\n",
    "print('For Ensemble SVM:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Ensemble: SVM and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hybrid Ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "clf1 = RandomForestClassifier(best_set_RF, random_state=1)\n",
    "clf2 = SVC(best_set_SVM, random_state=1)\n",
    "eclf1 = VotingClassifier(estimators=[('RF', clf1), ('SVM', clf2)], voting='hard')\n",
    "# hard = majority vote, soft = argmax of the sums of the predicted probabilities\n",
    "eclf1 = eclf1.fit(X_train,y_train)\n",
    "predicted_ens = eclf1.predict(X_test)\n",
    "y_pred = predicted_ens\n",
    "print('For Hybrid Ensemble:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

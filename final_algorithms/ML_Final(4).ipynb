{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Make the data set and labels\n",
    "# Create feature vector num. of samples x num. of features\n",
    "#X= np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n",
    "# Create feature vector num. of samples x 1 (bot or not)\n",
    "#y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])\n",
    "import pickle\n",
    "import pandas as pd\n",
    "#X_b = pd.read_pickle(\"bot_features.pkl\")\n",
    "#X_n = pd.read_pickle(\"nonbot_features.pkl\")\n",
    "X_b = np.load(open('bot_features.npy','rb'))\n",
    "X_n = np.load(open('nonbot_features.npy','rb'))\n",
    "X_b = [ row[1:] for row in X_b ]\n",
    "X_n = [ row[1:] for row in X_n ]\n",
    "print(np.size(X_b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modified from: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(y_test, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    cm = cnf_matrix\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    np.set_printoptions(precision=3)\n",
    "    plt.show()\n",
    "# plot_confusion_matrix(y_test, y_pred, classes=class_names,title='Confusion matrix (Normalize) ')\n",
    "class_names = ['bot', 'not-bot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modified from : http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "# y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "def plot_roc(y_test,y_score):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_score)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr[2], tpr[2], color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the Data is Processed the data is 75/25 split for train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data set for all classifiers first\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "y_b = np.ones(len(X_b))\n",
    "y_n = np.zeros(len(X_n))\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.25, random_state=42)\n",
    "X_train.extend(X_train_b)\n",
    "X_test.extend(X_test_b)\n",
    "y_train.extend(y_train_b)\n",
    "y_test.extend(y_test_b)\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(X_n, y_n, test_size=0.25, random_state=24)\n",
    "X_train.extend(X_train_n)\n",
    "X_test.extend(X_test_n)\n",
    "y_train.extend(y_train_n)\n",
    "y_test.extend(y_test_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Gaussian Naive Bayes:\n",
      "The accuracy is: 79.213483% \n",
      "The AUC score is: 0.799382 \n",
      "The precision is: 0.717391 \n",
      "The recall is: 0.920319 \n",
      "The f1 score is: 0.806283 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Naive Bayes for tweets\n",
    "# Create a Gaussian Classifier\n",
    "clf_NB = GaussianNB()\n",
    "# Train the model using the training sets \n",
    "clf_NB.fit(X_train, y_train)\n",
    "#Predict Output \n",
    "predicted_NB = clf_NB.predict(X_test)\n",
    "y_true = y_test\n",
    "y_pred = predicted_NB\n",
    "print('For Gaussian Naive Bayes:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_2f = np.array(clf_NB.predict(X_val))\n",
    "pred_3f = []\n",
    "for i in range (len(pred_2f)):\n",
    "    pred_3f.append([pred_2f[i]])\n",
    "pred_3f = np.array(pred_3f)\n",
    "print(pred_3f.shape)\n",
    "np.save('pred_NB.npy',pred_3f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Decision Tree:\n",
      "The accuracy is: 89.325843% \n",
      "The AUC score is: 0.891860 \n",
      "The precision is: 0.900826 \n",
      "The recall is: 0.868526 \n",
      "The f1 score is: 0.884381 \n",
      "{'max_depth': 8, 'min_samples_split': 0.0050000000000000001}\n"
     ]
    }
   ],
   "source": [
    "# Decison tree for tweets\n",
    "# Think about using PCA to simplfy data and have better DT\n",
    "# To find best Tree depth\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from math import floor\n",
    "# Should test depth from 3 to num. of features\n",
    "#param_grid = {'max_depth': np.arange(3, len(X_train[1])), 'min_samples_split':np.floor(len(y_train)*(np.arange(1,10,.5)/100))}\n",
    "param_grid = {'max_depth': np.arange(3, len(X_train[1])), 'min_samples_split':(np.arange(.5,10,.5)/100)}\n",
    "#param_grid = {'max_depth': np.arange(3, len(X_train[1])), 'min_samples_split':[5,10,12]}\n",
    "clf_DT = GridSearchCV(tree.DecisionTreeClassifier(), param_grid)\n",
    "clf_DT = clf_DT.fit(X_train, y_train)\n",
    "#predicted_DT = clf_DT.predict_proba(xtest)[:, 1]\n",
    "predicted_DT = clf_DT.predict(X_test)\n",
    "best_set = clf_DT.best_params_\n",
    "y_true = y_test\n",
    "y_pred = predicted_DT\n",
    "print('For Decision Tree:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))\n",
    "print(best_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Decision Tree:\n",
      "The accuracy is: 96.067416% \n",
      "The AUC score is: 0.961096 \n",
      "The precision is: 0.949219 \n",
      "The recall is: 0.968127 \n",
      "The f1 score is: 0.958580 \n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf_DT1 = tree.DecisionTreeClassifier(max_depth=6)\n",
    "clf_DT1 = clf_DT1.fit(X_train, y_train)\n",
    "#predicted_DT = clf_DT.predict_proba(xtest)[:, 1]\n",
    "predicted_DT1 = clf_DT1.predict(X_test)\n",
    "#best_set = clf_DT1.best_params_\n",
    "y_true = y_test\n",
    "y_pred = predicted_DT1\n",
    "print('For Decision Tree:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(clf_DT1.predict(X_val))\n",
    "pred_4f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_4f.append([pred_1f[i]])\n",
    "pred_4f = np.array(pred_4f)\n",
    "print(pred_4f.shape)\n",
    "np.save('pred_DT1.npy',pred_4f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Decision Tree with AdaBoost:\n",
      "The accuracy is: 90.823970% \n",
      "The AUC score is: 0.907121 \n",
      "The precision is: 0.913934 \n",
      "The recall is: 0.888446 \n",
      "The f1 score is: 0.901010 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "ada = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=8))\n",
    "ada = ada.fit(X_train,y_train) \n",
    "pred_ada = ada.predict(X_test)\n",
    "y_true = y_test\n",
    "y_pred = pred_ada\n",
    "print('For Decision Tree with AdaBoost:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n",
      "[[2281292622]\n",
      " [2344040251]\n",
      " [         765871267]\n",
      " [4772373433]\n",
      " [        1324548560]\n",
      " [2561341789]\n",
      " [         347810134]\n",
      " [         856303860]\n",
      " [832874807754502144]\n",
      " [          88856792]\n",
      " [713556609784479744]\n",
      " [        1566746503]\n",
      " [          90420314]\n",
      " [         184910040]\n",
      " [         157690631]\n",
      " [          42420346]\n",
      " [          42382447]\n",
      " [          43993280]\n",
      " [2305236733]\n",
      " [742793690838642688]\n",
      " [          31348594]\n",
      " [         122085859]\n",
      " [          23573083]\n",
      " [          43152482]\n",
      " [         188857501]\n",
      " [2911272579]\n",
      " [          35094637]\n",
      " [         146252766]\n",
      " [          85430866]\n",
      " [          55117855]\n",
      " [827941922941628420]\n",
      " [         234837526]\n",
      " [783181941524668416]\n",
      " [2236911966]\n",
      " [         238706610]\n",
      " [         537915105]\n",
      " [          19058681]\n",
      " [2568986838]\n",
      " [3002601144]\n",
      " [         515652246]\n",
      " [3411760894]\n",
      " [          34507480]\n",
      " [        1389883381]\n",
      " [         702010302]\n",
      " [          91425881]\n",
      " [         401790812]\n",
      " [          73992972]\n",
      " [         256722290]\n",
      " [            813286]\n",
      " [           5402612]\n",
      " [            742143]\n",
      " [         328539939]\n",
      " [         265430292]\n",
      " [        1257568902]\n",
      " [         132385468]\n",
      " [         211859777]\n",
      " [         827983208]\n",
      " [          72878717]\n",
      " [          28909448]\n",
      " [          43091065]\n",
      " [          50393960]\n",
      " [807343187337457664]\n",
      " [         372561152]\n",
      " [         385100876]\n",
      " [          14920785]\n",
      " [         783048890]\n",
      " [         274656070]\n",
      " [3041716769]\n",
      " [2452513406]\n",
      " [2575001119]\n",
      " [845988992361250818]\n",
      " [         854348514]\n",
      " [718744867015249920]\n",
      " [2419524854]\n",
      " [        1389274284]\n",
      " [705067031217856515]\n",
      " [842061193472835584]\n",
      " [          69274980]\n",
      " [         116000361]\n",
      " [        1281901753]\n",
      " [          26436076]\n",
      " [777990955085344768]\n",
      " [         350597282]\n",
      " [         153029178]\n",
      " [          64848375]\n",
      " [         165500655]\n",
      " [          16409683]\n",
      " [         142275422]\n",
      " [          21627372]\n",
      " [         106219812]\n",
      " [         203188566]\n",
      " [         100220864]\n",
      " [         331370908]\n",
      " [         280749815]\n",
      " [        1613648400]\n",
      " [        1473174745]\n",
      " [         456806128]\n",
      " [        1068121346]\n",
      " [2893075037]\n",
      " [         414311965]\n",
      " [         415798356]\n",
      " [          17782745]\n",
      " [         770626380]\n",
      " [748561245775728640]\n",
      " [         459483899]\n",
      " [         119509520]\n",
      " [         196267596]\n",
      " [         852763903]\n",
      " [3433157163]\n",
      " [            759251]\n",
      " [            428333]\n",
      " [        1478891947]\n",
      " [          18863815]\n",
      " [          94707303]\n",
      " [         115485051]\n",
      " [2727212335]\n",
      " [         237099368]\n",
      " [3353453309]\n",
      " [2716075003]\n",
      " [3254849422]\n",
      " [         155659213]\n",
      " [          17822889]\n",
      " [         115551858]\n",
      " [2216984384]\n",
      " [        1685674562]\n",
      " [          20978103]\n",
      " [          25521487]\n",
      " [          18695035]\n",
      " [         456862468]\n",
      " [          23976386]\n",
      " [         304932002]\n",
      " [         132309397]\n",
      " [         233532563]\n",
      " [         222622640]\n",
      " [          21111883]\n",
      " [        1249693405]\n",
      " [3472594037]\n",
      " [         101695592]\n",
      " [777041690842411009]\n",
      " [        1104552637]\n",
      " [         207247991]\n",
      " [          82066670]\n",
      " [          30268626]\n",
      " [          31127587]\n",
      " [775497972146589697]\n",
      " [834166973789458432]\n",
      " [          27195114]\n",
      " [2499849926]\n",
      " [         103768104]\n",
      " [          24946505]\n",
      " [2714673572]\n",
      " [          85452649]\n",
      " [4481535434]\n",
      " [         258092952]\n",
      " [         943358419]\n",
      " [2714207155]\n",
      " [         315519939]\n",
      " [          22940219]\n",
      " [         166739404]\n",
      " [          24568975]\n",
      " [         862483279]\n",
      " [        1360627633]\n",
      " [         173460266]\n",
      " [          21323153]\n",
      " [           2557521]\n",
      " [3105588872]\n",
      " [          67761261]\n",
      " [         858183698]\n",
      " [          96951800]\n",
      " [          43393110]\n",
      " [         565191043]\n",
      " [2969875259]\n",
      " [         234121917]\n",
      " [755647243676426240]\n",
      " [4915905545]\n",
      " [824944507917651968]\n",
      " [2571043022]\n",
      " [705785693155495937]\n",
      " [724026946603081728]\n",
      " [         318233704]\n",
      " [3405098242]\n",
      " [          20536157]\n",
      " [2565339643]\n",
      " [         163425239]\n",
      " [795777182186541056]\n",
      " [723974320029073408]\n",
      " [2629389268]\n",
      " [         168837454]\n",
      " [         784040629]\n",
      " [         204941435]\n",
      " [          93023732]\n",
      " [         181561712]\n",
      " [840687803520372736]\n",
      " [          53471080]\n",
      " [3762458901]\n",
      " [3297340437]\n",
      " [         529764127]\n",
      " [         854430271]\n",
      " [4438807636]\n",
      " [736498197221122048]\n",
      " [         738606434]\n",
      " [2546279960]\n",
      " [         391560579]\n",
      " [769629395262771200]\n",
      " [2756371298]\n",
      " [          58924265]\n",
      " [         542808418]\n",
      " [         101311381]\n",
      " [         189665290]\n",
      " [          92717138]\n",
      " [         113419517]\n",
      " [         169058185]\n",
      " [3433228216]\n",
      " [764398942259994628]\n",
      " [2857426909]\n",
      " [801419034692616193]\n",
      " [         180505807]\n",
      " [         843462523]\n",
      " [811381929601683456]\n",
      " [          14587132]\n",
      " [3229338322]\n",
      " [          18524364]\n",
      " [        1183779272]\n",
      " [         389576735]\n",
      " [2830252572]\n",
      " [         200179075]\n",
      " [          15485441]\n",
      " [4728875516]\n",
      " [         226956469]\n",
      " [          85603854]\n",
      " [          47177496]\n",
      " [3496516336]\n",
      " [         153130536]\n",
      " [         221097446]\n",
      " [         261025036]\n",
      " [          26565946]\n",
      " [        1627734804]\n",
      " [         219718241]\n",
      " [          20744091]\n",
      " [          27260086]\n",
      " [          60865434]\n",
      " [2935804025]\n",
      " [2555802726]\n",
      " [         169686021]\n",
      " [2716014782]\n",
      " [765575886414368770]\n",
      " [         459660480]\n",
      " [767029329171709952]\n",
      " [          21447363]\n",
      " [701328508191006720]\n",
      " [721525882788134916]\n",
      " [         632775923]\n",
      " [2574905460]\n",
      " [         157140968]\n",
      " [         102424662]\n",
      " [3020652335]\n",
      " [          23151437]\n",
      " [2925761350]\n",
      " [          32959253]\n",
      " [          25365536]\n",
      " [         850755931]\n",
      " [          23083404]\n",
      " [836270435784237056]\n",
      " [3020873886]\n",
      " [          23617610]\n",
      " [         401269376]\n",
      " [         236699098]\n",
      " [          14230524]\n",
      " [         333055559]\n",
      " [         123343918]\n",
      " [         133880286]\n",
      " [          96904590]\n",
      " [768980704361377792]\n",
      " [769247835283922952]\n",
      " [         158314798]\n",
      " [3153783803]\n",
      " [         116362700]\n",
      " [         122555708]\n",
      " [         470792609]\n",
      " [         498549828]\n",
      " [          84279963]\n",
      " [2713738866]\n",
      " [         522264595]\n",
      " [          82375729]\n",
      " [          57113624]\n",
      " [         845343720]\n",
      " [2774502312]\n",
      " [800721193795883008]\n",
      " [828977030632869889]\n",
      " [2572483813]\n",
      " [3229574074]\n",
      " [739491456344170497]\n",
      " [          19248106]\n",
      " [2567072732]\n",
      " [          90748957]\n",
      " [3229405733]\n",
      " [          19670467]\n",
      " [         416030450]\n",
      " [3245582815]\n",
      " [         449436847]\n",
      " [758468707144327172]\n",
      " [         159966364]\n",
      " [         382972481]\n",
      " [          27106837]\n",
      " [         860795611]\n",
      " [         106010958]\n",
      " [         244743317]\n",
      " [         168477019]\n",
      " [        1403581621]\n",
      " [         268414482]\n",
      " [2381315456]\n",
      " [4054357513]\n",
      " [         375117674]\n",
      " [         244910581]\n",
      " [3364413982]\n",
      " [754892109216296961]\n",
      " [         219255067]\n",
      " [          64459603]\n",
      " [         164481132]\n",
      " [         465182549]\n",
      " [         133404204]\n",
      " [739961370767069184]\n",
      " [         850495242]\n",
      " [         852739632]\n",
      " [          26658046]\n",
      " [          52775726]\n",
      " [          18839785]\n",
      " [          11348282]\n",
      " [          17471979]\n",
      " [         253321807]\n",
      " [          36107106]\n",
      " [          19923144]\n",
      " [         556142994]\n",
      " [          18393773]\n",
      " [2314672190]\n",
      " [780707721209188352]\n",
      " [         158487331]\n",
      " [          19426551]\n",
      " [         105119490]\n",
      " [          59775065]\n",
      " [          55752358]\n",
      " [          35787166]\n",
      " [         407616007]\n",
      " [          22342898]\n",
      " [         256634583]\n",
      " [2561646679]\n",
      " [4091727471]\n",
      " [3290651747]\n",
      " [3426729850]\n",
      " [            807095]\n",
      " [         306097862]\n",
      " [         352376426]\n",
      " [3229554058]\n",
      " [4095361159]\n",
      " [         129942953]\n",
      " [759024715348541440]\n",
      " [         858204450]\n",
      " [          15888142]\n",
      " [3298173622]\n",
      " [         209708391]\n",
      " [          19397785]\n",
      " [         845589283]\n",
      " [          84083187]\n",
      " [         856242686]\n",
      " [         589535421]\n",
      " [          71978637]\n",
      " [4855815726]\n",
      " [3020309903]\n",
      " [         165731902]\n",
      " [         903162031]\n",
      " [         375694319]\n",
      " [          23652567]\n",
      " [         123322294]\n",
      " [          28706024]\n",
      " [          31927467]\n",
      " [        1273164162]\n",
      " [3901064081]\n",
      " [         471741741]\n",
      " [728138615180529664]\n",
      " [822215679726100480]\n",
      " [          44252420]\n",
      " [          18681139]\n",
      " [         145046142]\n",
      " [          35408870]\n",
      " [3137971624]\n",
      " [         856077606]\n",
      " [768783064277323776]\n",
      " [4639760414]\n",
      " [         190550701]\n",
      " [3265528788]\n",
      " [2556842574]\n",
      " [4883207059]\n",
      " [2564337723]\n",
      " [         205203060]\n",
      " [         716530098]\n",
      " [          25073877]\n",
      " [          14872237]\n",
      " [750763541251186689]\n",
      " [         270426592]\n",
      " [4886553186]\n",
      " [711122580141248512]\n",
      " [768569546760388608]\n",
      " [750867989491290113]\n",
      " [           1652541]\n",
      " [         150817706]\n",
      " [          40255499]\n",
      " [          22338630]\n",
      " [          79293791]\n",
      " [2467190329]\n",
      " [         480575795]\n",
      " [         854422044]\n",
      " [          16190898]\n",
      " [2420565234]\n",
      " [         194538554]\n",
      " [         373745691]\n",
      " [          22166136]\n",
      " [         160591075]\n",
      " [         311830293]\n",
      " [         342347053]\n",
      " [          83462293]\n",
      " [         114976173]\n",
      " [832038744525926401]\n",
      " [         190591939]\n",
      " [          20206051]\n",
      " [         549264466]\n",
      " [         291227271]\n",
      " [2205024632]\n",
      " [        1620743941]\n",
      " [         356486086]\n",
      " [          23375688]\n",
      " [3072915819]\n",
      " [           1715051]\n",
      " [        1968714288]\n",
      " [3199884297]\n",
      " [          44409004]\n",
      " [         524956084]\n",
      " [846011302044577792]\n",
      " [        1384640706]\n",
      " [         804182444]\n",
      " [         175883705]\n",
      " [          70727809]\n",
      " [         854482440]\n",
      " [         919979270]\n",
      " [         819531865]\n",
      " [4746905184]\n",
      " [         850590698]\n",
      " [2600705052]\n",
      " [           3004231]\n",
      " [738925282707406848]\n",
      " [4925299425]\n",
      " [2174746250]\n",
      " [705215650986332162]\n",
      " [          26257166]\n",
      " [         145125358]\n",
      " [          86231642]\n",
      " [          45052382]\n",
      " [3300622553]\n",
      " [2259884294]\n",
      " [2468086038]\n",
      " [2342447737]\n",
      " [2262148759]\n",
      " [3694685310]\n",
      " [         847608391]\n",
      " [         249218671]\n",
      " [         841111291]\n",
      " [         772554582]\n",
      " [          98572780]\n",
      " [        1076033178]\n",
      " [2759909491]\n",
      " [          17919972]\n",
      " [2354507203]\n",
      " [          97762747]\n",
      " [788563925327568896]\n",
      " [          48626235]\n",
      " [           5988062]\n",
      " [          15846407]\n",
      " [          21623813]\n",
      " [3631838303]\n",
      " [3341453481]\n",
      " [3433099685]\n",
      " [4919025952]\n",
      " [3224552461]\n",
      " [2577008935]\n",
      " [         424308815]\n",
      " [2572529604]\n",
      " [2838688612]\n",
      " [        1360710618]\n",
      " [         160951141]\n",
      " [        1032940160]\n",
      " [3406693534]\n",
      " [812529080998432769]\n",
      " [734899922776903680]\n",
      " [737117265972367361]\n",
      " [791768681340547072]\n",
      " [757037167646347266]\n",
      " [791083162021470208]\n",
      " [807330926724268032]\n",
      " [777743872885657600]\n",
      " [839727016047964162]\n",
      " [824362393539067909]\n",
      " [809360606344122368]\n",
      " [752664756461371392]\n",
      " [800692856239230976]\n",
      " [785874738832773120]\n",
      " [841399567375585280]\n",
      " [776463226389291016]\n",
      " [779804994191056896]\n",
      " [758313909837266945]\n",
      " [741843516725329920]\n",
      " [729192762407444480]\n",
      " [750576871398596608]\n",
      " [761184223483031552]\n",
      " [832746992212140033]\n",
      " [829359125389770756]\n",
      " [745020724612579329]\n",
      " [836731331207819264]\n",
      " [761738131594850305]\n",
      " [788956924406562816]\n",
      " [844297308225355776]\n",
      " [731212486116941824]\n",
      " [736442344153583616]\n",
      " [826345519865794561]\n",
      " [812009731011190785]\n",
      " [734440531212673024]\n",
      " [780693810477043713]\n",
      " [814728881575968768]\n",
      " [760099111899201536]\n",
      " [832137223264927744]\n",
      " [831133282926546944]\n",
      " [753536959373508608]\n",
      " [841789787640086528]\n",
      " [743973864535785472]\n",
      " [771372189237010432]\n",
      " [816412011647090688]\n",
      " [732713969132019713]\n",
      " [787298926923812864]\n",
      " [800159143193550849]\n",
      " [809853812496277504]\n",
      " [797553451500793856]\n",
      " [727712724818530305]\n",
      " [2900085166]\n",
      " [            783214]\n",
      " [          85426644]\n",
      " [         360945774]\n",
      " [2986306048]\n",
      " [731201272376926208]\n",
      " [         612754791]\n",
      " [4493562022]\n",
      " [2897136909]\n",
      " [          78956001]\n",
      " [4462343293]\n",
      " [3830053332]\n",
      " [         586671909]\n",
      " [         332888068]\n",
      " [756936918328348672]\n",
      " [2163813157]\n",
      " [2602312513]\n",
      " [812814257779773440]\n",
      " [         355883433]\n",
      " [3229506502]\n",
      " [          20322929]\n",
      " [          16712547]\n",
      " [         268439864]\n",
      " [2566951536]\n",
      " [         174632702]\n",
      " [         843270408]\n",
      " [2564439320]\n",
      " [         342737458]\n",
      " [         566078011]\n",
      " [         268809577]\n",
      " [750998690245054464]\n",
      " [          10228272]\n",
      " [         218833868]\n",
      " [         176566242]\n",
      " [3119554528]\n",
      " [          14802766]\n",
      " [          14802766]]\n",
      "(577L, 1L)\n",
      "[[2281292622          0]\n",
      " [2344040251          1]\n",
      " [ 765871267          0]\n",
      " ..., \n",
      " [3119554528          1]\n",
      " [  14802766          0]\n",
      " [  14802766          0]]\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "X_id = np.load(open('test_ids.npy','rb'))\n",
    "#X_id = pickle.load('test_ids.pkl')\n",
    "X_id = X_id.astype(np.int64)\n",
    "print(X_id.shape)\n",
    "print(X_id)\n",
    "pred_ada = np.array(ada.predict(X_val))\n",
    "pred2 = []\n",
    "for i in range (len(pred_ada)):\n",
    "    pred2.append([pred_ada[i]])\n",
    "pred2 = np.array(pred2)\n",
    "print(pred2.shape)\n",
    "#np.save('pred.npy',pred2)\n",
    "#outset = [X_id,pred_ada]\n",
    "#outset = np.append((X_id,pred2),0)\n",
    "outset = np.column_stack((X_id,pred2))\n",
    "#np.set_printoptions(suppress=True)\n",
    "print(outset.astype(np.int64))\n",
    "#np.savetxt(\"pred.csv\", outset, delimiter=\",\")\n",
    "# np.savetxt(\"pred.csv\", outset, fmt=\"%10s %10.3f\")\n",
    "#outset.tofile(\"pred2.csv\",sep=',')\n",
    "#np.savetxt(\"pred.csv\", (X_id.T,pred_ada.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pf = pd.DataFrame(outset)\n",
    "pf.to_csv('pred5.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pydotplus",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-0e65611ed11c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;31m# To visualize the tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'has_bot_string'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'account_age'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'followers'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'favourites'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'contributors'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'friends'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'geo'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'has_extended_profile'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'is_translation_enabled'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'lang'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'location'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'notifications'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tweets'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'verified'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'default_profile'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'default_profile_image'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'listed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'bot'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'not_bot'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pydotplus"
     ]
    }
   ],
   "source": [
    "clf_DT2 = tree.DecisionTreeClassifier(max_depth=8,min_samples_split= 0.014999999999999999)\n",
    "clf_DT2 = clf_DT2.fit(X_train, y_train)\n",
    "# To visualize the tree\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "names = ['has_bot_string','account_age','followers','favourites','contributors','friends','geo','has_extended_profile','is_translation_enabled','lang','location','notifications','tweets','verified','url','default_profile','default_profile_image','listed']\n",
    "clss = ['bot','not_bot']\n",
    "dot_data = tree.export_graphviz(clf_DT2, out_file=None, \n",
    "                     feature_names=names,  \n",
    "                     class_names=clss,  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-08274aa66bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m               \u001b[1;34m\"bootstrap\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m               \"criterion\": [\"gini\", \"entropy\"]}\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mgrid_search_RF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_RF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mgrid_search_RF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mpredicted_RF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search_RF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_RF = RandomForestClassifier()\n",
    "# param_grid = {\"max_depth\": [np.arange(3, len(X_train[1])), None],\n",
    "#               \"min_samples_split\": (len(y_train)*(np.arange(0,11,.5)/100)),\n",
    "#               \"bootstrap\": [True, False],\n",
    "#               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "param_grid = {#\"max_depth\": [np.arange(3, len(X_train[1])), None],\n",
    "              #\"max_depth\": [3,  4,  5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, None],\n",
    "              \"min_samples_split\": (np.arange(.5,10,.5)/100),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "grid_search_RF = GridSearchCV(clf_RF, param_grid=param_grid)\n",
    "grid_search_RF.fit(X_train, y_train)\n",
    "predicted_RF = grid_search_RF.predict(X_test)\n",
    "y_pred = predicted_RF\n",
    "print('For Random Forest:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))\n",
    "best_set_RF = grid_search_RF.best_params_\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=10):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "report(grid_search_RF.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_RF = np.array(grid_search_RF.predict(X_val))\n",
    "pred_RF2 = []\n",
    "for i in range (len(pred_RF)):\n",
    "    pred_RF2.append([pred_RF[i]])\n",
    "pred_RF2 = np.array(pred_RF2)\n",
    "print(pred_RF2.shape)\n",
    "np.save('pred_RF2.npy',pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SVM:\n",
      "The accuracy is: 89.325843% \n",
      "The AUC score is: 0.895464 \n",
      "The precision is: 0.854015 \n",
      "The recall is: 0.932271 \n",
      "The f1 score is: 0.891429 \n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "# recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], \n",
    "# or standardize it to have mean 0 and variance 1\n",
    "from sklearn.svm import SVC\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['poly'], 'degree' : np.arange(1,6), 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['sigmoid'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}]\n",
    "scores = ['precision', 'recall']\n",
    "score = scores[1]\n",
    "clf_SVM = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "clf_SVM.fit(X_train, y_train)\n",
    "best_set_SVM = clf_SVM.best_params_\n",
    "predicted_SVM = clf_SVM.predict(X_test)\n",
    "y_pred = predicted_SVM\n",
    "print('For SVM:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_SVM3 = np.array(clf_SVM.predict(X_val))\n",
    "pred_SVM4 = []\n",
    "for i in range (len(pred_SVM3)):\n",
    "    pred_SVM4.append([pred_SVM3[i]])\n",
    "pred_SVM4 = np.array(pred_SVM4)\n",
    "print(pred_SVM4.shape)\n",
    "np.save('pred_SVM4.npy',pred_SVM4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915887850467\n",
      "0.911214953271\n",
      "0.897196261682\n",
      "0.906103286385\n",
      "0.915492957746\n",
      "0.882629107981\n",
      "0.87323943662\n",
      "0.896713615023\n",
      "0.868544600939\n",
      "0.859154929577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsb = np.array_split(X_train_b,n_sets)\\nsn = np.array_split(X_train_n,n_sets)\\nsbc = np.array_split(y_train_b,n_sets)\\nsnc = np.array_split(y_train_n,n_sets)\\nfor i in range(0,n_sets):\\n    sb[i]=np.concatenate((sb[i], sn[i]), axis=0)\\n    sbc[i]=np.concatenate((sbc[i], snc[i]), axis=0)\\nsvm1.fit(sb[0],sbc[1])\\nsvm2.fit(sb[1],sbc[2])\\nsvm3.fit(sb[2],sbc[3])\\nsvm4.fit(sb[3],sbc[4])\\nsvm5.fit(sb[4],sbc[5])\\nsvm6.fit(sb[5],sbc[6])\\nsvm7.fit(sb[6],sbc[6])\\nsvm8.fit(sb[7],sbc[7])\\nsvm9.fit(sb[8],sbc[8])\\nsvm10.fit(sb[9],sbc[9])\\nvt1 = svm1.predict(X_test)\\nvt2 = svm2.predict(X_test)\\nvt3 = svm3.predict(X_test)\\nvt4 = svm4.predict(X_test)\\nvt5 = svm5.predict(X_test)\\nvt6 = svm6.predict(X_test)\\nvt7 = svm7.predict(X_test)\\nvt8 = svm8.predict(X_test)\\nvt9 = svm9.predict(X_test)\\nvt10 = svm10.predict(X_test)\\nvt_count = np.array(vt1)+np.array(vt2)+np.array(vt3)+np.array(vt4)+np.array(vt5)+np.array(vt6)+np.array(vt7)+np.array(vt8)+np.array(vt9)+np.array(vt10)\\npredicted_ES = [ vt_count[i]>=5 for i in range(0,len(vt_count)) ]\\ny_pred = predicted_ES\\nprint(\\'For Ensemble SVM:\\')\\nacc = accuracy_score(y_true, y_pred)\\nprint (\"The accuracy is: %f%% \" % ( 100*acc ))\\nauc = (roc_auc_score(y_true, y_pred))\\nprint (\"The AUC score is: %f \" %  ( auc ))\\npre = metrics.precision_score(y_true, y_pred)\\nprint (\"The precision is: %f \" %  ( pre ))\\nrec = metrics.recall_score(y_true, y_pred)\\nprint (\"The recall is: %f \" %  ( rec ))\\nf1 = metrics.f1_score(y_true, y_pred)  \\nprint (\"The f1 score is: %f \" %  ( f1 ))\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EnsembleSVM\n",
    "# Want to randomly sample data np.random.shuffle(x)\n",
    "# And then trian multiple SVM's \n",
    "# Hard vote on a label\n",
    "#print(best_set_SVM)\n",
    "from math import floor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#best_set_SVM = C=10, kernel=linear,\n",
    "n_sets = 10\n",
    "#nsn = floor(len(X_train_n)/n_sets)\n",
    "#nsb = floor(len(X_train_b)/n_sets)\n",
    "svm1 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm2 = SVC(C=10, kernel='linear', random_state=2)\n",
    "svm3 = SVC(C=10, kernel='linear', random_state=3)\n",
    "svm4 = SVC(C=10, kernel='linear', random_state=4)\n",
    "svm5 = SVC(C=10, kernel='linear', random_state=5)\n",
    "svm6 = SVC(C=10, kernel='linear', random_state=6)\n",
    "svm7 = SVC(C=10, kernel='linear', random_state=7)\n",
    "svm8 = SVC(C=10, kernel='linear', random_state=8)\n",
    "svm9 = SVC(C=10, kernel='linear', random_state=9)\n",
    "svm10 = SVC(C=10, kernel='linear', random_state=10)\n",
    "SVMs = [svm1,svm2,svm3,svm4,svm5,svm6,svm7,svm9,svm10]\n",
    "k_fold = KFold(n_splits=n_sets,shuffle=True)\n",
    "#k_fold = StratifiedKFold(n_splits=n_sets,shuffle=True)\n",
    "X = X_train+X_test #.extend(X_test)\n",
    "X = np.array(X)\n",
    "#print(X)\n",
    "y = y_train+y_test #.extend(y_test)\n",
    "y = np.array(y)\n",
    "# print(len(y))\n",
    "#k = 0;\n",
    "for k, (train, test) in enumerate(k_fold.split(X, y)):\n",
    "    sv = SVC(C=10, kernel='linear', random_state=k)\n",
    "    #SVMs[k] = SVMs[k].fit(X[train], y[train])\n",
    "    sv.fit(X[train], y[train])\n",
    "    #print(\"[fold {0}], score: {2:.5f}\".format(k, SVMs[k].score(X[test], y[test])))\n",
    "    #print(\"[fold {0}], score: {2:.5f}\".format(k, sv.score(X[test], y[test])))\n",
    "    print(sv.score(X[test], y[test]))\n",
    "    #k=k+1\n",
    "'''\n",
    "sb = np.array_split(X_train_b,n_sets)\n",
    "sn = np.array_split(X_train_n,n_sets)\n",
    "sbc = np.array_split(y_train_b,n_sets)\n",
    "snc = np.array_split(y_train_n,n_sets)\n",
    "for i in range(0,n_sets):\n",
    "    sb[i]=np.concatenate((sb[i], sn[i]), axis=0)\n",
    "    sbc[i]=np.concatenate((sbc[i], snc[i]), axis=0)\n",
    "svm1.fit(sb[0],sbc[1])\n",
    "svm2.fit(sb[1],sbc[2])\n",
    "svm3.fit(sb[2],sbc[3])\n",
    "svm4.fit(sb[3],sbc[4])\n",
    "svm5.fit(sb[4],sbc[5])\n",
    "svm6.fit(sb[5],sbc[6])\n",
    "svm7.fit(sb[6],sbc[6])\n",
    "svm8.fit(sb[7],sbc[7])\n",
    "svm9.fit(sb[8],sbc[8])\n",
    "svm10.fit(sb[9],sbc[9])\n",
    "vt1 = svm1.predict(X_test)\n",
    "vt2 = svm2.predict(X_test)\n",
    "vt3 = svm3.predict(X_test)\n",
    "vt4 = svm4.predict(X_test)\n",
    "vt5 = svm5.predict(X_test)\n",
    "vt6 = svm6.predict(X_test)\n",
    "vt7 = svm7.predict(X_test)\n",
    "vt8 = svm8.predict(X_test)\n",
    "vt9 = svm9.predict(X_test)\n",
    "vt10 = svm10.predict(X_test)\n",
    "vt_count = np.array(vt1)+np.array(vt2)+np.array(vt3)+np.array(vt4)+np.array(vt5)+np.array(vt6)+np.array(vt7)+np.array(vt8)+np.array(vt9)+np.array(vt10)\n",
    "predicted_ES = [ vt_count[i]>=5 for i in range(0,len(vt_count)) ]\n",
    "y_pred = predicted_ES\n",
    "print('For Ensemble SVM:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_SVM3 = np.array(sv.predict(X_val))\n",
    "pred_SVM4 = []\n",
    "for i in range (len(pred_SVM3)):\n",
    "    pred_SVM4.append([pred_SVM3[i]])\n",
    "pred_SVM4 = np.array(pred_SVM4)\n",
    "print(pred_SVM4.shape)\n",
    "np.save('pred_sv.npy',pred_SVM4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Ensemble: SVM and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Hybrid Ensemble:\n",
      "The accuracy is: 96.441948% \n",
      "The AUC score is: 0.962377 \n",
      "The precision is: 0.995726 \n",
      "The recall is: 0.928287 \n",
      "The f1 score is: 0.960825 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b6f9440d9a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"The f1 score is: %f \"\u001b[0m \u001b[1;33m%\u001b[0m  \u001b[1;33m(\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Confusion matrix (Normalize) '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Hybrid Ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "clf1 = RandomForestClassifier( random_state=1)\n",
    "clf2 = SVC(C=10, kernel='linear', random_state=1)\n",
    "eclf1 = VotingClassifier(estimators=[('RF', clf1), ('SVM', clf2)], voting='hard')\n",
    "# hard = majority vote, soft = argmax of the sums of the predicted probabilities\n",
    "eclf1 = eclf1.fit(X_train,y_train)\n",
    "predicted_ens = eclf1.predict(X_test)\n",
    "y_pred = predicted_ens\n",
    "print('For Hybrid Ensemble:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))\n",
    "plot_confusion_matrix(y_true, y_pred, classes=class_names,title='Confusion matrix (Normalize) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(eclf1.predict(X_val))\n",
    "pred_4f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_4f.append([pred_1f[i]])\n",
    "pred_4f = np.array(pred_4f)\n",
    "print(pred_4f.shape)\n",
    "np.save('pred_ECL.npy',pred_4f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ensemble SVM 0:\n",
      "The accuracy is: 67.228464% \n",
      "For Ensemble SVM 1:\n",
      "The accuracy is: 85.205993% \n",
      "For Ensemble SVM 2:\n",
      "The accuracy is: 66.666667% \n",
      "For Ensemble SVM 3:\n",
      "The accuracy is: 52.996255% \n",
      "For Ensemble SVM 4:\n",
      "The accuracy is: 52.996255% \n",
      "For Ensemble SVM 5:\n",
      "The accuracy is: 52.996255% \n",
      "For Ensemble SVM 6:\n",
      "The accuracy is: 69.662921% \n",
      "For Ensemble SVM 7:\n",
      "The accuracy is: 56.367041% \n",
      "For Ensemble SVM 8:\n",
      "The accuracy is: 52.996255% \n",
      "For Ensemble SVM 9:\n",
      "The accuracy is: 52.996255% \n",
      "For Ensemble SVM 10:\n",
      "The accuracy is: 62.921348% \n",
      "For Ensemble SVM 11:\n",
      "The accuracy is: 52.996255% \n",
      "For Ensemble SVM 12:\n",
      "The accuracy is: 55.617978% \n",
      "For Ensemble SVM 13:\n",
      "The accuracy is: 68.726592% \n",
      "For Ensemble SVM 14:\n",
      "The accuracy is: 56.367041% \n",
      "For Ensemble SVM 15:\n",
      "The accuracy is: 69.101124% \n",
      "For Ensemble SVM 16:\n",
      "The accuracy is: 54.119850% \n",
      "For Ensemble SVM 17:\n",
      "The accuracy is: 64.794007% \n"
     ]
    }
   ],
   "source": [
    "# Modified EnsembleSVM\n",
    "# Split data per column\n",
    "# And then trian multiple SVM's, 1 per feature \n",
    "# Hard vote on a label\n",
    "#print(best_set_SVM)\n",
    "from sklearn.svm import SVC\n",
    "#best_set_SVM = C=10, kernel=linear,\n",
    "# Same random state for better control\n",
    "svm1 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm2 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm3 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm4 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm5 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm6 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm7 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm8 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm9 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm10 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm11 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm12 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm13 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm14 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm15 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm16 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm17 = SVC(C=10, kernel='linear', random_state=1)\n",
    "svm18 = SVC(C=10, kernel='linear', random_state=1)\n",
    "X_train2 = np.array(X_train)\n",
    "svm1.fit(X_train2[:,0].reshape(-1, 1),y_train)\n",
    "svm2.fit(X_train2[:,1].reshape(-1, 1),y_train)\n",
    "svm3.fit(X_train2[:,2].reshape(-1, 1),y_train)\n",
    "svm4.fit(X_train2[:,3].reshape(-1, 1),y_train)\n",
    "svm5.fit(X_train2[:,4].reshape(-1, 1),y_train)\n",
    "svm6.fit(X_train2[:,5].reshape(-1, 1),y_train)\n",
    "svm7.fit(X_train2[:,6].reshape(-1, 1),y_train)\n",
    "svm8.fit(X_train2[:,7].reshape(-1, 1),y_train)\n",
    "svm9.fit(X_train2[:,8].reshape(-1, 1),y_train)\n",
    "svm10.fit(X_train2[:,9].reshape(-1, 1),y_train)\n",
    "svm11.fit(X_train2[:,10].reshape(-1, 1),y_train)\n",
    "svm12.fit(X_train2[:,11].reshape(-1, 1),y_train)\n",
    "svm13.fit(X_train2[:,12].reshape(-1, 1),y_train)\n",
    "svm14.fit(X_train2[:,13].reshape(-1, 1),y_train)\n",
    "svm15.fit(X_train2[:,14].reshape(-1, 1),y_train)\n",
    "svm16.fit(X_train2[:,15].reshape(-1, 1),y_train)\n",
    "svm17.fit(X_train2[:,16].reshape(-1, 1),y_train)\n",
    "svm18.fit(X_train2[:,17].reshape(-1, 1),y_train)\n",
    "vt = []\n",
    "X_test2 = np.array(X_test)\n",
    "vt.append(svm1.predict(X_test2[:,0].reshape(-1, 1)))\n",
    "vt.append(svm2.predict(X_test2[:,1].reshape(-1, 1)))\n",
    "vt.append(svm3.predict(X_test2[:,2].reshape(-1, 1)))\n",
    "vt.append(svm4.predict(X_test2[:,3].reshape(-1, 1)))\n",
    "vt.append(svm5.predict(X_test2[:,4].reshape(-1, 1)))\n",
    "vt.append(svm6.predict(X_test2[:,5].reshape(-1, 1)))\n",
    "vt.append(svm7.predict(X_test2[:,6].reshape(-1, 1)))\n",
    "vt.append(svm8.predict(X_test2[:,7].reshape(-1, 1)))\n",
    "vt.append(svm9.predict(X_test2[:,8].reshape(-1, 1)))\n",
    "vt.append(svm10.predict(X_test2[:,9].reshape(-1, 1)))\n",
    "vt.append(svm11.predict(X_test2[:,10].reshape(-1, 1)))\n",
    "vt.append(svm12.predict(X_test2[:,11].reshape(-1, 1)))\n",
    "vt.append(svm13.predict(X_test2[:,12].reshape(-1, 1)))\n",
    "vt.append(svm14.predict(X_test2[:,13].reshape(-1, 1)))\n",
    "vt.append(svm15.predict(X_test2[:,14].reshape(-1, 1)))\n",
    "vt.append(svm16.predict(X_test2[:,15].reshape(-1, 1)))\n",
    "vt.append(svm17.predict(X_test2[:,16].reshape(-1, 1)))\n",
    "vt.append(svm18.predict(X_test2[:,17].reshape(-1, 1)))\n",
    "\"\"\"\n",
    "vt_count = np.array(vt1)+np.array(vt2)+np.array(vt3)+np.array(vt4)+np.array(vt5)+np.array(vt6)+np.array(vt7)+np.array(vt8)+np.array(vt9)+np.array(vt10)\n",
    "predicted_ES = [ vt_count[i]>=5 for i in range(0,len(vt_count)) ]\n",
    "y_pred = predicted_ES\n",
    "print('For Ensemble SVM:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))\n",
    "\"\"\"\n",
    "for i in range(18):\n",
    "    print('For Ensemble SVM %i:' % i)\n",
    "    acc = accuracy_score(y_test, vt[i])\n",
    "    print (\"The accuracy is: %f%% \" % ( 100*acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Ensemble SVM:\n",
      "The accuracy is: 83.707865% \n"
     ]
    }
   ],
   "source": [
    "vt_count = np.array(vt[0])+np.array(vt[1])+np.array(vt[2])+np.array(vt[6])+np.array(vt[10])+np.array(vt[13])+np.array(vt[15])+np.array(vt[17])\n",
    "predicted_ES = [ vt_count[i]>=6 for i in range(0,len(vt_count)) ]\n",
    "y_pred = predicted_ES\n",
    "print('For Ensemble SVM:')\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 85.515395% \n"
     ]
    }
   ],
   "source": [
    "y_pred = svm2.predict(X_train2[:,1].reshape(-1, 1))\n",
    "acc = accuracy_score(y_train, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(svm2.predict(X_val[:,1].reshape(-1, 1)))\n",
    "pred_2f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_2f.append([pred_1f[i]])\n",
    "pred_2f = np.array(pred_2f)\n",
    "print(pred_2f.shape)\n",
    "np.save('pred_1f.npy',pred_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 24, 62, 64, 86, 92, 96, 99, 101, 115, 117, 120, 125, 146, 157, 160, 167, 169, 180, 192, 206, 220, 222, 226, 257, 263, 285, 300, 315, 318, 322, 325, 333, 335, 348, 364, 376, 377, 381, 384, 389, 405, 409, 414, 448, 453, 501, 527, 529, 555, 560, 564, 573, 574, 575, 582, 593, 595, 605, 616, 634, 639, 656, 659, 679, 683, 685, 689, 713, 717, 745, 748, 752, 768, 796, 797, 819, 820, 876, 898, 932, 956, 959, 966, 972, 995]\n",
      "[5, 6, 7, 10, 14, 16, 22, 37, 38, 40, 51, 52, 58, 62, 68, 71, 81, 82, 87, 92, 100, 102, 113, 114, 120, 127, 132, 133, 142, 145, 147, 148, 156, 158, 161, 164, 167, 169, 171, 173, 178, 179, 192, 195, 197, 207, 215, 216, 219, 245, 251, 255, 260, 263, 267, 271, 275, 282, 296, 297, 300, 307, 310, 312, 313, 315, 321, 325, 327, 329, 341, 343, 355, 365, 367, 374, 380, 385, 391, 400, 401, 402, 406, 417, 418, 419, 424, 431, 433, 438, 442, 447, 456, 461, 465, 476, 481, 489, 492, 493, 496, 497, 499, 504, 517, 540, 541, 543, 551, 552, 564, 572, 580, 587, 590, 617, 622, 627, 639, 650, 653, 657, 660, 663, 665, 671, 677, 678, 681, 684, 690, 695, 699, 708, 715, 717, 725, 731, 735, 737, 755, 761, 762, 766, 768, 769, 770, 779, 785, 793, 799, 806, 813, 820, 824, 832, 841, 850, 854, 855, 866, 868, 869, 873, 882, 886, 889, 894, 897, 902, 907, 908, 910, 911, 914, 924, 936, 941, 944, 947, 954, 956, 961, 964, 965, 970, 971, 973, 975, 986, 989, 1001, 1002]\n"
     ]
    }
   ],
   "source": [
    "X_b2 = np.array(X_b)\n",
    "X_n2 = np.array(X_n)\n",
    "y_pred = svm2.predict(X_b2[:,1].reshape(-1, 1))\n",
    "y_pred2 = svm2.predict(X_n2[:,1].reshape(-1, 1))\n",
    "bad_bot = []\n",
    "bad_not = []\n",
    "for i in range(len(y_pred)):\n",
    "    if (y_pred[i] != 1):\n",
    "        bad_bot.append(i)\n",
    "for i in range(len(y_pred)):\n",
    "    if (y_pred2[i] != 0):\n",
    "        bad_not.append(i)\n",
    "print(bad_bot)\n",
    "print(bad_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SVM:\n",
      "The accuracy is: 87.827715% \n",
      "The AUC score is: 0.880204 \n",
      "The precision is: 0.841912 \n",
      "The recall is: 0.912351 \n",
      "The f1 score is: 0.875717 \n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "# recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], \n",
    "# or standardize it to have mean 0 and variance 1\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['poly'], 'degree' : np.arange(1,6), 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['sigmoid'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]}]\n",
    "scores = ['precision', 'recall']\n",
    "score = scores[1]\n",
    "clf_SVM = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "X_test2 = np.array(X_test)\n",
    "X_train2 = np.array(X_train)\n",
    "clf_SVM.fit(X_train2[:,[0,1,2,6,10,13,15,17]], y_train)\n",
    "best_set_SVM = clf_SVM.best_params_\n",
    "predicted_SVM = clf_SVM.predict(X_test2[:,[0,1,2,6,10,13,15,17]])\n",
    "y_pred = predicted_SVM\n",
    "y_true = y_test\n",
    "print('For SVM:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_SVM = np.array(clf_SVM.predict(X_val[:,[0,1,2,6,10,13,15,17]]))\n",
    "pred_SVM2 = []\n",
    "for i in range (len(pred_SVM)):\n",
    "    pred_SVM2.append([pred_SVM[i]])\n",
    "pred_SVM2 = np.array(pred_SVM2)\n",
    "print(pred_SVM2.shape)\n",
    "np.save('pred_SVM2.npy',pred_SVM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(clf_SVM.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.87719298  0.8994152   0.90621336  0.9073857   0.90152403]\n",
      "Accuracy: 0.90 (+/- 0.02)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.model_selection import cross_val_predict\\npredicted = cross_val_predict(clf, iris.data, iris.target, cv=10)\\nmetrics.accuracy_score(iris.target, predicted) \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = SVC(kernel='rbf', C=1000, gamma=0.01)\n",
    "X_train_v = np.array(X_train.extend(X_test)), \n",
    "y_train_v = np.array(y_train.extend(y_test))\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "'''\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "predicted = cross_val_predict(clf, iris.data, iris.target, cv=10)\n",
    "metrics.accuracy_score(iris.target, predicted) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7e945f2256b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "scores.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For SVM:\n",
      "The accuracy is: 87.640449% \n",
      "The AUC score is: 0.878662 \n",
      "The precision is: 0.836364 \n",
      "The recall is: 0.916335 \n",
      "The f1 score is: 0.874525 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-2], 'C': [ 100, 1000,10000]}]\n",
    "scores = ['precision', 'recall']\n",
    "score = scores[1]\n",
    "clf_SVM = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "X_test2 = np.array(X_test)\n",
    "X_train2 = np.array(X_train)\n",
    "clf_SVM.fit(X_train2[:,[0,1,2,6,10,13,15,17]], y_train)\n",
    "best_set_SVM = clf_SVM.best_params_\n",
    "predicted_SVM = clf_SVM.predict(X_test2[:,[0,1,2,6,10,13,15,17]])\n",
    "y_pred = predicted_SVM\n",
    "y_true = y_test\n",
    "print('For SVM:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print(clf_SVM.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(clf_SVM.predict(X_val[:,[0,1,2,6,10,13,15,17]]))\n",
    "pred_2f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_2f.append([pred_1f[i]])\n",
    "pred_2f = np.array(pred_2f)\n",
    "print(pred_2f.shape)\n",
    "np.save('pred_2f.npy',pred_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MLP:\n",
      "The accuracy is: 88.014981% \n",
      "The AUC score is: 0.881520 \n",
      "The precision is: 0.850187 \n",
      "The recall is: 0.904382 \n",
      "The f1 score is: 0.876448 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#tuned_parameters = [{'hidden_layer_sizes':[(20,10,5)],'activation' : ['identity', 'logistic', 'tanh', 'relu'], 'solver' : ['lbfgs', 'sgd', 'adam'],'alpha': [.1,.01,.001,.0001], 'learning_rate' : ['constant', 'invscaling', 'adaptive']}]\n",
    "scores = ['precision', 'recall']\n",
    "score = scores[1]\n",
    "#verbose='true'\n",
    "#clf_MLP = GridSearchCV(MLPClassifier(), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "clf_MLP = MLPClassifier(hidden_layer_sizes = (5), activation = 'relu', solver = 'sgd')\n",
    "clf_MLP.fit(X_train, y_train)\n",
    "predicted_MLP = clf_MLP.predict(X_test)\n",
    "y_pred = predicted_MLP\n",
    "y_true = y_test\n",
    "print('For MLP:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MLP:\n",
      "The accuracy is: 47.013352% \n",
      "The AUC score is: 0.500000 \n",
      "The precision is: 0.470134 \n",
      "The recall is: 1.000000 \n",
      "The f1 score is: 0.639579 \n"
     ]
    }
   ],
   "source": [
    "predicted_MLPt = clf_MLP.predict(X_train)\n",
    "y_pred = predicted_MLPt\n",
    "y_true = y_train\n",
    "print('For MLP:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3760\n",
      "3760\n",
      "3760\n"
     ]
    }
   ],
   "source": [
    "indeces = (clf_MLP.predict(X_train) == y_train)\n",
    "#indeces = indeces.tolist()\n",
    "X_train = np.array(X_train)\n",
    "X_train_new = X_train[indeces]\n",
    "y_train = np.array(y_train)\n",
    "y_train_new = y_train[indeces]\n",
    "print(len(X_train_new))\n",
    "print(len(y_train_new))\n",
    "print(sum(indeces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MLP:\n",
      "The accuracy is: 98.829787% \n",
      "The AUC score is: 0.988570 \n",
      "The precision is: 0.979448 \n",
      "The recall is: 0.996698 \n",
      "The f1 score is: 0.987998 \n"
     ]
    }
   ],
   "source": [
    "clf_MLP = MLPClassifier(hidden_layer_sizes = (5), activation = 'relu', solver = 'sgd')\n",
    "clf_MLP.fit(X_train_new, y_train_new)\n",
    "predicted_MLP = clf_MLP.predict(X_train_new)\n",
    "y_pred = predicted_MLP\n",
    "y_true = y_train_new\n",
    "print('For MLP:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(clf_MLP.predict(X_val))\n",
    "pred_2f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_2f.append([pred_1f[i]])\n",
    "pred_2f = np.array(pred_2f)\n",
    "print(pred_2f.shape)\n",
    "np.save('pred_MLP_88.npy',pred_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4269L, 18L)\n",
      "(2007L, 18L)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(X_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1]\n",
      "[ 9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2,3,4,5,6,7,8,9,1,1,1,1])\n",
    "z = np.array([1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "y = x[x==1]\n",
    "d = z[x==1]\n",
    "print(y)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Decision Tree with AdaBoost:\n",
      "The accuracy is: 100.000000% \n",
      "The AUC score is: 1.000000 \n",
      "The precision is: 1.000000 \n",
      "The recall is: 1.000000 \n",
      "The f1 score is: 1.000000 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "ada2 = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=8))\n",
    "ada2 = ada2.fit(X_train_new,y_train_new) \n",
    "pred_ada2 = ada2.predict(X_train_new)\n",
    "y_true = y_train_new\n",
    "y_pred = pred_ada2\n",
    "print('For Decision Tree with AdaBoost:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.470133520731\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(predicted_MLP)/len(predicted_MLP))\n",
    "print(np.sum(y_train_new)/len(y_train_new))\n",
    "print(np.sum(y_train)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(ada2.predict(X_val))\n",
    "pred_2f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_2f.append([pred_1f[i]])\n",
    "pred_2f = np.array(pred_2f)\n",
    "print(pred_2f.shape)\n",
    "np.save('pred_ada2.npy',pred_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For GradientBoostingClassifier:\n",
      "The accuracy is: 98.314607% \n",
      "The AUC score is: 0.982973 \n",
      "The precision is: 0.984000 \n",
      "The recall is: 0.980080 \n",
      "The f1 score is: 0.982036 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc = gbc.fit(X_train,y_train) \n",
    "pred_gbc = gbc.predict(X_test)\n",
    "y_true = y_test\n",
    "y_pred = pred_gbc\n",
    "print('For GradientBoostingClassifier:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For GradientBoostingClassifier:\n",
      "The accuracy is: 99.812734% \n",
      "The AUC score is: 0.998233 \n",
      "The precision is: 0.996032 \n",
      "The recall is: 1.000000 \n",
      "The f1 score is: 0.998012 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn = knn.fit(X_train, y_train) \n",
    "pred_knn = knn.predict(X_test)\n",
    "y_true = y_test\n",
    "y_pred = pred_knn\n",
    "print('For KNN-\n",
    "      Classifier:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(knn.predict(X_val))\n",
    "pred_2f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_2f.append([pred_1f[i]])\n",
    "pred_2f = np.array(pred_2f)\n",
    "print(pred_2f.shape)\n",
    "np.save('pred_knn.npy',pred_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For KNN-Classifier:\n",
      "The accuracy is: 88.014981% \n",
      "The AUC score is: 0.881970 \n",
      "The precision is: 0.845018 \n",
      "The recall is: 0.912351 \n",
      "The f1 score is: 0.877395 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn2 = KNeighborsClassifier(n_neighbors=100)\n",
    "X_train2 = np.array(X_train)\n",
    "X_test2 = np.array(X_test)\n",
    "knn2 = knn2.fit(X_train2[:,[0,1,2,6,10,13,15,17]], y_train) \n",
    "pred_knn2 = knn2.predict(X_test2[:,[0,1,2,6,10,13,15,17]])\n",
    "y_true = y_test\n",
    "y_pred = pred_knn2\n",
    "print('For KNN-Classifier:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For MLP:\n",
      "The accuracy is: 86.891386% \n",
      "The AUC score is: 0.871595 \n",
      "The precision is: 0.824373 \n",
      "The recall is: 0.916335 \n",
      "The f1 score is: 0.867925 \n"
     ]
    }
   ],
   "source": [
    "X_train2 = np.array(X_train)\n",
    "X_test2 = np.array(X_test)\n",
    "clf_MLP2 = MLPClassifier(hidden_layer_sizes = (5), activation = 'relu', solver = 'sgd')\n",
    "clf_MLP2.fit(X_train2[:,[0,1,2,6,10,13,15,17]], y_train)\n",
    "predicted_MLP2 = clf_MLP2.predict(X_test2[:,[0,1,2,6,10,13,15,17]])\n",
    "y_pred = predicted_MLP2\n",
    "y_true = y_test\n",
    "print('For MLP:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_sgd = np.array(clf_MLP2.predict(X_val[:,[0,1,2,6,10,13,15,17]]))\n",
    "pred_sgd2 = []\n",
    "for i in range (len(pred_sgd)):\n",
    "    pred_sgd2.append([pred_sgd[i]])\n",
    "pred_sgd2 = np.array(pred_sgd2)\n",
    "print(pred_sgd2.shape)\n",
    "np.save('pred_sgd4.npy',pred_sgd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Decision Tree:\n",
      "The accuracy is: 87.827715% \n",
      "The AUC score is: 0.880429 \n",
      "The precision is: 0.839416 \n",
      "The recall is: 0.916335 \n",
      "The f1 score is: 0.876190 \n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X_train2 = np.array(X_train)\n",
    "X_test2 = np.array(X_test)\n",
    "clf_DT3 = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf_DT3 = clf_DT3.fit(X_train2[:,[0,1,2,6,10,13,15,17]], y_train)\n",
    "#predicted_DT = clf_DT.predict_proba(xtest)[:, 1]\n",
    "predicted_DT3 = clf_DT3.predict(X_test2[:,[0,1,2,6,10,13,15,17]])\n",
    "#best_set = clf_DT1.best_params_\n",
    "y_true = y_test\n",
    "y_pred = predicted_DT3\n",
    "print('For Decision Tree:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Kmeans:\n",
      "The accuracy is: 74.396814% \n",
      "The AUC score is: 0.757669 \n",
      "The precision is: 0.649934 \n",
      "The recall is: 0.987045 \n",
      "The f1 score is: 0.783778 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array(X_train)\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++', n_init=20, random_state=0).fit(X)\n",
    "y = kmeans.labels_\n",
    "yt = kmeans.predict(np.array(X_test))\n",
    "# kmeans.cluster_centers_\n",
    "y_true = y_test\n",
    "#y_true = y_train\n",
    "y_pred = 1-yt\n",
    "print('For Kmeans:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.713984539688\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_pred)/(len(y_pred)+.0000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Decision Tree:\n",
      "The accuracy is: 87.453184% \n",
      "The AUC score is: 0.876895 \n",
      "The precision is: 0.833333 \n",
      "The recall is: 0.916335 \n",
      "The f1 score is: 0.872865 \n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "X_train2 = np.array(X_train)\n",
    "X_test2 = np.array(X_test)\n",
    "clf_DT3 = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf_DT3 = clf_DT3.fit(X_train2[:,[0,1,2,6,13,15]], y_train)\n",
    "#predicted_DT = clf_DT.predict_proba(xtest)[:, 1]\n",
    "predicted_DT3 = clf_DT3.predict(X_test2[:,[0,1,2,6,13,15]])\n",
    "#best_set = clf_DT1.best_params_\n",
    "y_true = y_test\n",
    "y_pred = predicted_DT3\n",
    "print('For Decision Tree:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(clf_DT3.predict(X_val[:,[0,1,2,6,13,15]]))\n",
    "pred_2f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_2f.append([pred_1f[i]])\n",
    "pred_2f = np.array(pred_2f)\n",
    "print(pred_2f.shape)\n",
    "np.save('pred_tl.npy',pred_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Decision Tree:\n",
      "The accuracy is: 84.082397% \n",
      "The AUC score is: 0.841940 \n",
      "The precision is: 0.812030 \n",
      "The recall is: 0.860558 \n",
      "The f1 score is: 0.835590 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X_train2 = np.array(X_train)\n",
    "X_test2 = np.array(X_test)\n",
    "clf_RFl = RandomForestClassifier(n_estimators=25)\n",
    "clf_RFl = clf_RFl.fit(X_train2[:,[0,1,2,6,13,15]], y_train)\n",
    "#predicted_DT = clf_DT.predict_proba(xtest)[:, 1]\n",
    "predicted_RFl = clf_RFl.predict(X_test2[:,[0,1,2,6,13,15]])\n",
    "#best_set = clf_DT1.best_params_\n",
    "y_true = y_test\n",
    "y_pred = predicted_RFl\n",
    "print('For Decision Tree:')\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print (\"The accuracy is: %f%% \" % ( 100*acc ))\n",
    "auc = (roc_auc_score(y_true, y_pred))\n",
    "print (\"The AUC score is: %f \" %  ( auc ))\n",
    "pre = metrics.precision_score(y_true, y_pred)\n",
    "print (\"The precision is: %f \" %  ( pre ))\n",
    "rec = metrics.recall_score(y_true, y_pred)\n",
    "print (\"The recall is: %f \" %  ( rec ))\n",
    "f1 = metrics.f1_score(y_true, y_pred)  \n",
    "print (\"The f1 score is: %f \" %  ( f1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(577L, 1L)\n"
     ]
    }
   ],
   "source": [
    "X_val = np.load(open('test_features.npy','rb'))\n",
    "pred_1f = np.array(clf_RFl.predict(X_val[:,[0,1,2,6,13,15]]))\n",
    "pred_2f = []\n",
    "for i in range (len(pred_1f)):\n",
    "    pred_2f.append([pred_1f[i]])\n",
    "pred_2f = np.array(pred_2f)\n",
    "print(pred_2f.shape)\n",
    "np.save('pred_rfl.npy',pred_2f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
